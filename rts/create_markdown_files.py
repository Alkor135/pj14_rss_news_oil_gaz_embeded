"""
1. –°–∫—Ä–∏–ø—Ç –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç markdown-—Ñ–∞–π–ª—ã —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏, —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–æ —Ç–æ—Ä–≥–æ–≤—ã–º —Å–µ—Å—Å–∏—è–º.
2. –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö ‚Äî SQLite –ë–î —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏ (`rss_news_*.db`) –∏ –±–∏—Ä–∂–µ–≤—ã–º–∏ –¥–∞—Ç–∞–º–∏ —Ç–æ—Ä–≥–æ–≤.
3. –ù–æ–≤–æ—Å—Ç–∏ –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –≤ –µ–¥–∏–Ω—ã–π DataFrame –∏ —Å–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–≥—Ä—É–∑–∫–∏.
4. –¢–æ—Ä–≥–æ–≤—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã —Å—Ç—Ä–æ—è—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞—Ç –∏–∑ —Ç–∞–±–ª–∏—Ü—ã `Futures` (–ø—Ä–µ–¥—ã–¥—É—â–∞—è + —Ç–µ–∫—É—â–∞—è –¥–∞—Ç–∞).
5. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ —Å–æ–∑–¥–∞—ë—Ç—Å—è `.md` —Ñ–∞–π–ª —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏, –ø–æ–ø–∞–≤—à–∏–º–∏ –≤ –µ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏.
6. –ü–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ —É–¥–∞–ª—è–µ—Ç—Å—è —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π markdown-—Ñ–∞–π–ª.
7. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —á–µ—Ä–µ–∑ `settings.yaml` –∏ —Ä–æ—Ç–∞—Ü–∏—è –ª–æ–≥–æ–≤.
"""

import pandas as pd
from pathlib import Path
import sqlite3
import logging
from logging.handlers import TimedRotatingFileHandler
import yaml
from datetime import datetime

SETTINGS_FILE = Path(__file__).parent / "settings.yaml"

with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
    settings = yaml.safe_load(f)

ticker = settings['ticker']
ticker_lc = ticker.lower()
num_mds = settings['num_mds']  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ (–¥–Ω–µ–π) –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ markdown —Ñ–∞–π–ª—ã
num_dbs = settings['num_dbs']
time_start = settings['time_start']
time_end = settings['time_end']
path_db_day = Path(settings['path_db_day'].replace('{ticker}', ticker))
db_news_dir = Path(settings['db_news_dir'])
md_path = Path(settings['md_path'])
provider = settings['provider']

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è –ª–æ–≥–æ–≤
log_dir = Path(__file__).parent / 'log'
log_dir.mkdir(parents=True, exist_ok=True)

# –ò–º—è —Ñ–∞–π–ª–∞ –ª–æ–≥–∞ —Å –¥–∞—Ç–æ–π –∏ –≤—Ä–µ–º–µ–Ω–µ–º –∑–∞–ø—É—Å–∫–∞ (–æ–¥–∏–Ω —Ñ–∞–π–ª –Ω–∞ –∑–∞–ø—É—Å–∫!)
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
log_file = log_dir / f'create_markdown_files_{timestamp}.txt'

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: –¢–û–õ–¨–ö–û –æ–¥–∏–Ω —Ñ–∞–π–ª + –∫–æ–Ω—Å–æ–ª—å
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),  # –æ–¥–∏–Ω —Ñ–∞–π–ª
        logging.StreamHandler()                           # –∫–æ–Ω—Å–æ–ª—å
    ]
)

# –†—É—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ 3 —Å–∞–º—ã—Ö –Ω–æ–≤—ã—Ö)
def cleanup_old_logs(log_dir: Path, max_files: int = 3):
    """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –ª–æ–≥-—Ñ–∞–π–ª—ã, –æ—Å—Ç–∞–≤–ª—è—è max_files —Å–∞–º—ã—Ö –Ω–æ–≤—ã—Ö."""
    log_files = sorted(log_dir.glob("create_markdown_files_*.txt"))
    if len(log_files) > max_files:
        for old_file in log_files[:-max_files]:
            try:
                old_file.unlink()
                print(f"–£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π –ª–æ–≥: {old_file.name}")
            except Exception as e:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {old_file}: {e}")

# –í—ã–∑—ã–≤–∞–µ–º –æ—á–∏—Å—Ç–∫—É –ü–ï–†–ï–î –Ω–∞—á–∞–ª–æ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
cleanup_old_logs(log_dir, max_files=3)
logging.info(f"üöÄ –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞. –õ–æ–≥-—Ñ–∞–π–ª: {log_file}")

def read_news_dbs_to_df(db_dir: Path, num_dbs: int | None = None) -> pd.DataFrame:
    """
    –ß–∏—Ç–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤ SQLite –ë–î —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ db_dir
    –≤ –æ–¥–∏–Ω DataFrame –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç –ø–æ –ø–æ–ª—é loaded_at.

    –û–∂–∏–¥–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–æ–≤: rss_news_YYYY_MM.db
    –ö–æ–ª–æ–Ω–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ: loaded_at, date, title, provider.
    """
    db_files = sorted(db_dir.glob("rss_news_*.db"))

    if num_dbs is not None and num_dbs > 0:
        db_files = db_files[-num_dbs:]  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ num_dbs —Ñ–∞–π–ª–æ–≤

    all_rows = []

    for db_file in db_files:
        try:
            with sqlite3.connect(db_file) as conn:
                df_part = pd.read_sql_query(
                    "SELECT loaded_at, date, title, provider FROM news",
                    conn
                )
                df_part["source_db"] = db_file.name  # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –æ—Ç–∫—É–¥–∞ —Å—Ç—Ä–æ–∫–∞
                all_rows.append(df_part)
            logging.info(f"–£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω —Ñ–∞–π–ª –ë–î: {db_file}")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ë–î {db_file}: {e}")

    if not all_rows:
        logging.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –ë–î –Ω–æ–≤–æ—Å—Ç–µ–π")
        return pd.DataFrame(columns=["loaded_at", "date", "title", "provider", "source_db"])

    df_all = pd.concat(all_rows, ignore_index=True)

    # –í—ã–±–æ—Ä —Å—Ç—Ä–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º (investing, prime, interfax).
    if provider=='investing':
        df_all = df_all[df_all['provider'].str.contains('investing', case=False, na=False)]
    elif provider=='prime_interfax':
        df_all = df_all[df_all['provider'].str.contains('interfax|prime', case=False, na=False)]

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ: "–Ω–µ—Ñ—Ç—å" –∏–ª–∏ "–≥–∞–∑"
    keywords_pattern = r'–Ω–µ—Ñ—Ç|–≥–∞–∑'
    df_all = df_all[df_all['title'].str.contains(keywords_pattern, case=False, na=False)]
    logging.info(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(df_all)} –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º '–Ω–µ—Ñ—Ç' –∏–ª–∏ '–≥–∞–∑'")

    # –ü—Ä–∏–≤–æ–¥–∏–º loaded_at –∫ datetime –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ë–î
    df_all["loaded_at"] = pd.to_datetime(df_all["loaded_at"])
    df_all = df_all.sort_values(["loaded_at", "provider", "title"]).reset_index(drop=True)

    return df_all

def build_trade_intervals(
    db_path: str,
    time_start: str = '21:00:00',
    time_end: str = '20:59:59',
    table_name: str = "Futures"
):
    """
    –ß–∏—Ç–∞–µ—Ç –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É TRADEDATE –∏–∑ SQLite-–ë–î –∏ —Å—Ç—Ä–æ–∏—Ç –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã:
    (prev_date + time_start, curr_date + time_end).

    –ü—Ä–∏–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:
    (
        (datetime(2025, 6, 2, 21, 0), datetime(2025, 6, 3, 20, 59, 59)),
        (datetime(2025, 6, 3, 21, 0), datetime(2025, 6, 4, 20, 59, 59)),
        ...
    )
    """
    with sqlite3.connect(db_path) as conn:
        cur = conn.cursor()
        cur.execute(f"SELECT TRADEDATE FROM {table_name} ORDER BY TRADEDATE")
        rows = cur.fetchall()

    # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–æ–∫ –¥–∞—Ç (str)
    dates = [r[0] for r in rows]

    # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º –¥–≤–µ –¥–∞—Ç—ã –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
    if len(dates) < 2:
        return tuple()

    intervals = []

    for prev_date_str, curr_date_str in zip(dates[:-1], dates[1:]):
        # –°–∫–ª–µ–∏–≤–∞–µ–º –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è –∏ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ datetime
        start_dt = datetime.fromisoformat(f"{prev_date_str} {time_start}")
        end_dt = datetime.fromisoformat(f"{curr_date_str} {time_end}")
        intervals.append((start_dt, end_dt))

    return tuple(intervals)

def create_markdown_files_from_intervals(
    df_news: pd.DataFrame,
    intervals: tuple,
    md_dir: Path,
    ticker: str,
) -> None:
    """
    –ü–æ –∫–∞–∂–¥–æ–º—É –∏–Ω—Ç–µ—Ä–≤–∞–ª—É (start_dt, end_dt) –∏–∑ intervals
    —Å–æ–∑–¥–∞—ë—Ç markdown-—Ñ–∞–π–ª —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ df_news.title,
    —É –∫–æ—Ç–æ—Ä—ã—Ö loaded_at –ø–æ–ø–∞–¥–∞–µ—Ç –≤ —ç—Ç–æ—Ç –∏–Ω—Ç–µ—Ä–≤–∞–ª.

    –ò–º—è —Ñ–∞–π–ª–∞: YYYY-MM-DD.md, –≥–¥–µ –¥–∞—Ç–∞ –±–µ—Ä—ë—Ç—Å—è –∏–∑ end_dt —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–∞—Ä—ã.
    –ü–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤:
    - —É–¥–∞–ª—è–µ—Ç—Å—è —Å–∞–º—ã–π –ø–æ–∑–¥–Ω–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π md-—Ñ–∞–π–ª –ø–æ –¥–∞—Ç–µ –≤ –∏–º–µ–Ω–∏;
    - –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∏—Ö –µ—â—ë –Ω–µ—Ç.
    """
    md_dir.mkdir(parents=True, exist_ok=True)

    # ==== 1. –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–π –ø–æ—Å–ª–µ–¥–Ω–∏–π md-—Ñ–∞–π–ª –ø–æ –¥–∞—Ç–µ –≤ –∏–º–µ–Ω–∏ ====
    md_files = sorted(md_dir.glob("*.md"))
    if md_files:
        # –û–∂–∏–¥–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –∏–º–µ–Ω–∏: YYYY-MM-DD.md
        def extract_date(p: Path):
            try:
                return datetime.fromisoformat(p.stem).date()
            except ValueError:
                return None

        dated_files = [(extract_date(p), p) for p in md_files]
        dated_files = [(d, p) for d, p in dated_files if d is not None]
        if dated_files:
            last_date, last_path = max(dated_files, key=lambda x: x[0])
            try:
                last_path.unlink()
                logging.info(f"–£–¥–∞–ª—ë–Ω –ø–æ—Å–ª–µ–¥–Ω–∏–π markdown-—Ñ–∞–π–ª: {last_path}")
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è markdown-—Ñ–∞–π–ª–∞ {last_path}: {e}")

    # ==== 2. –°–æ–∑–¥–∞—ë–º —Ç–æ–ª—å–∫–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã ====

    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ loaded_at –≤ datetime
    if not pd.api.types.is_datetime64_any_dtype(df_news["loaded_at"]):
        df_news = df_news.copy()
        df_news["loaded_at"] = pd.to_datetime(df_news["loaded_at"])

    for start_dt, end_dt in intervals:
        # –ò–º—è —Ñ–∞–π–ª–∞ –ø–æ –¥–∞—Ç–µ –∫–æ–Ω—Ü–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
        date_str = end_dt.date().isoformat()
        filename = f"{date_str}.md"
        filepath = md_dir / filename

        # –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        if filepath.exists():
            logging.info(f"Markdown-—Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫: {filepath}")
            continue

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª—É
        mask = (df_news["loaded_at"] >= start_dt) & (df_news["loaded_at"] <= end_dt)
        df_slice = df_news.loc[mask].sort_values("loaded_at")

        if df_slice.empty:
            continue  # –Ω–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π ‚Äî —Ñ–∞–π–ª –Ω–µ —Å–æ–∑–¥–∞—ë–º

        lines = []
        for _, row in df_slice.iterrows():
            title = str(row["title"])
            lines.append(title)
            lines.append("")  # –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞-—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å

        content = "\n".join(lines)

        try:
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(content)
            logging.info(f"–°–æ–∑–¥–∞–Ω markdown-—Ñ–∞–π–ª: {filepath}")
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ markdown-—Ñ–∞–π–ª–∞ {filepath}: {e}")

if __name__ == "__main__":
    df_news = read_news_dbs_to_df(db_news_dir, num_dbs=num_dbs)
    with pd.option_context(  # –ü–µ—á–∞—Ç—å —à–∏—Ä–æ–∫–æ–≥–æ –∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞
            "display.width", 1000,
            "display.max_columns", 30,
            "display.max_colwidth", 100
    ):
        print("–î–∞—Ç–∞—Ñ—Ä–µ–π–º —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º:")
        print(df_news)

    intervals = build_trade_intervals(
        db_path=path_db_day,  # –∏–∑ settings.yaml
        time_start=time_start,
        time_end=time_end,
        table_name="Futures"  # –¢–∞–±–ª–∏—Ü–∞ –≤ –ë–î –∫–æ—Ç–∏—Ä–æ–≤–æ–∫
    )
    for it in intervals[:5]:
        print(it)

    create_markdown_files_from_intervals(
        df_news=df_news,
        intervals=intervals[-num_mds:],  # –Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ num_mds –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
        md_dir=md_path,
        ticker=ticker,
    )
    print(f"\nMarkdown —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã –≤ {md_path}")