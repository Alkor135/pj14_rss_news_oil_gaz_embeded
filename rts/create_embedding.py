"""
–°–æ–∑–¥–∞—ë—Ç –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω—ã—Ö markdown-–æ—Ç—á—ë—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Ollama (–ª–æ–∫–∞–ª—å–Ω–æ) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª–∏ bge-m3, qwen3-embedding –∏–ª–∏ embeddinggemma.
–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ —Ç–æ–∫–µ–Ω–∞–º —Å —É—á—ë—Ç–æ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä—ã L2.
–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ MD5 –∏ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –æ—Ç—á—ë—Ç—ã.
–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ pickle-—Ñ–∞–π–ª –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∏ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∫—Ä–∏–ø—Ç–∞—Ö.
–õ–æ–≥–∏—Ä—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–æ—Ç–∞—Ü–∏–µ–π (–æ—Å—Ç–∞–≤–ª—è–µ—Ç 3 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –ª–æ–≥–∞).
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∏—Ä–∏–ª–ª–∏—Ü—É, –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø—É—Å—Ç—ã–µ –∏ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã.
"""

from pathlib import Path
import pickle
import hashlib
import numpy as np
from chromadb.utils.embedding_functions import OllamaEmbeddingFunction
import logging
import yaml
from datetime import datetime
import pandas as pd
import tiktoken
import sys
import time

# –ü—É—Ç—å –∫ settings.yaml –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, —á—Ç–æ –∏ —Å–∫—Ä–∏–ø—Ç
SETTINGS_FILE = Path(__file__).parent / "settings.yaml"

# –ß—Ç–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫
with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
    settings = yaml.safe_load(f)

# ==== –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ====
ticker = settings['ticker']
ticker_lc = ticker.lower()
url_ai = settings.get('url_ai', 'http://localhost:11434/api/embeddings')  # Ollama API –±–µ–∑ —Ç–∞–π–º-–∞—É—Ç–∞
model_name = settings.get('model_name', 'embeddinggemma')  # Ollama –º–æ–¥–µ–ª—å
md_path = Path(settings['md_path'])  # –ü—É—Ç—å –∫ markdown-—Ñ–∞–π–ª–∞–º
if model_name == 'bge-m3':
    # max_chunk_tokens = 7000  # –î–ª—è bge-m3 (8192 –ª–∏–º–∏—Ç –º–∏–Ω—É—Å –∑–∞–ø–∞—Å)
    max_chunk_tokens = 800  # retriever-grade
elif model_name == 'qwen3-embedding:0.6b':
    # max_chunk_tokens = 32000  # –î–ª—è qwen3-embedding:0.6b (32768 –ª–∏–º–∏—Ç –º–∏–Ω—É—Å –∑–∞–ø–∞—Å)
    max_chunk_tokens = 1000  # retriever-grade
elif model_name == 'embeddinggemma':
    #max_chunk_tokens = 1800  # –î–ª—è embeddinggemma (2048 –ª–∏–º–∏—Ç –º–∏–Ω—É—Å –∑–∞–ø–∞—Å)
    max_chunk_tokens = 512  # retriever-grade
    # max_chunk_tokens = 200  # –ø—Ä–æ–±–∞ –Ω–∏–∑–∫–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
else:
    print('–ü—Ä–æ–≤–µ—Ä—å –º–æ–¥–µ–ª—å')
    sys.exit()

# –ü—É—Ç—å –∫ pkl-—Ñ–∞–π–ª—É —Å –∫—ç—à–µ–º
# cache_file = Path(settings['cache_file'].replace('{ticker_lc}', ticker_lc))
cache_file = Path(__file__).parent / 'embeddings_ollama.pkl'

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è –ª–æ–≥–æ–≤
log_dir = Path(__file__).parent / 'log'
log_dir.mkdir(parents=True, exist_ok=True)

# –ò–º—è —Ñ–∞–π–ª–∞ –ª–æ–≥–∞ —Å –¥–∞—Ç–æ–π –∏ –≤—Ä–µ–º–µ–Ω–µ–º –∑–∞–ø—É—Å–∫–∞ (–æ–¥–∏–Ω —Ñ–∞–π–ª –Ω–∞ –∑–∞–ø—É—Å–∫!)
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
log_file = log_dir / f'create_embedding_ollama_{timestamp}.txt'

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: –¢–û–õ–¨–ö–û –æ–¥–∏–Ω —Ñ–∞–π–ª + –∫–æ–Ω—Å–æ–ª—å
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),  # –æ–¥–∏–Ω —Ñ–∞–π–ª
        logging.StreamHandler()                           # –∫–æ–Ω—Å–æ–ª—å
    ]
)

# –†—É—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ 3 —Å–∞–º—ã—Ö –Ω–æ–≤—ã—Ö)
def cleanup_old_logs(log_dir: Path, max_files: int = 3):
    """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –ª–æ–≥-—Ñ–∞–π–ª—ã, –æ—Å—Ç–∞–≤–ª—è—è max_files —Å–∞–º—ã—Ö –Ω–æ–≤—ã—Ö."""
    log_files = sorted(log_dir.glob("create_embedding_ollama_*.txt"))
    if len(log_files) > max_files:
        for old_file in log_files[:-max_files]:
            try:
                old_file.unlink()
                print(f"–£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π –ª–æ–≥: {old_file.name}")
            except Exception as e:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {old_file}: {e}")

# –í—ã–∑—ã–≤–∞–µ–º –æ—á–∏—Å—Ç–∫—É –ü–ï–†–ï–î –Ω–∞—á–∞–ª–æ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
cleanup_old_logs(log_dir, max_files=3)
logging.info(f"üöÄ –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞. –õ–æ–≥-—Ñ–∞–π–ª: {log_file}")

enc = tiktoken.get_encoding("cl100k_base")

def token_len(text: str) -> int:
    return len(enc.encode(text))

# === –§—É–Ω–∫—Ü–∏—è –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ Ollama ===
ef = OllamaEmbeddingFunction(model_name=model_name)

def load_existing_cache(cache_file: Path) -> pd.DataFrame | None:
    if cache_file.exists():
        try:
            with open(cache_file, "rb") as f:
                df = pickle.load(f)
            logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫—ç—à: {cache_file}, —Å—Ç—Ä–æ–∫: {len(df)}")
            return df
        except Exception as e:
            logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à {cache_file}: {e}")
    return None

def build_embeddings_df(md_dir: Path, existing_df: pd.DataFrame | None) -> pd.DataFrame:
    """
    –°–æ–∑–¥–∞—ë—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏:
    TRADEDATE (–¥–∞—Ç–∞ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ YYYY-MM-DD.md),
    MD5_hash (md5 —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞),
    VECTORS (—ç–º–±–µ–¥–¥–∏–Ω–≥ —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ OllamaEmbeddingFunction).
    """
    # –°–æ–∑–¥–∞—ë–º lookup –ø–æ TRADEDATE –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫—ç—à–∞
    cache_lookup = {}
    if existing_df is not None and not existing_df.empty:
        cache_lookup = {
            row["TRADEDATE"]: {
                "MD5_hash": row["MD5_hash"],
                "CHUNKS": row["CHUNKS"],
            }
            for _, row in existing_df.iterrows()
        }

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª–æ–≤–∞—Ä—å, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å TRADEDATE
    result_dict = {}  # TRADEDATE -> {"MD5_hash": ..., "CHUNKS": ...}

    md_files = sorted(md_dir.glob("*.md"))
    logging.info(f"–ù–∞–π–¥–µ–Ω–æ markdown-—Ñ–∞–π–ª–æ–≤: {len(md_files)}")

    for md_file in md_files:
        file_start_time = time.perf_counter()  # ‚è±Ô∏è –°—Ç–∞—Ä—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞

        try:
            tradedate_str = md_file.stem  # 'YYYY-MM-DD'
        except Exception as e:
            logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞—Ç—É –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ {md_file.name}: {e}")
            continue

        try:
            text = md_file.read_text(encoding='utf-8')
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {md_file}: {e}")
            continue

        if not text.strip():
            logging.info(f"–ü—É—Å—Ç–æ–π —Ñ–∞–π–ª, –ø—Ä–æ–ø—É—Å–∫: {md_file}")
            continue

        # MD5-—Ö—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        md5_hash = hashlib.md5(text.encode('utf-8')).hexdigest()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ —Ñ–∞–π–ª
        cached = cache_lookup.get(tradedate_str)

        if cached and cached["MD5_hash"] == md5_hash:
            # –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π ‚Äî –±–µ—Ä—ë–º –∏–∑ –∫—ç—à–∞
            result_dict[tradedate_str] = {
                "TRADEDATE": tradedate_str,
                "MD5_hash": md5_hash,
                "CHUNKS": cached["CHUNKS"],
            }
            file_time = time.perf_counter() - file_start_time
            logging.info(f"{md_file.name}: –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –≤–∑—è—Ç–æ –∏–∑ –∫—ç—à–∞. –í—Ä–µ–º—è: {file_time:.2f} —Å–µ–∫.")
            continue

        # === –§–∞–π–ª –∏–∑–º–µ–Ω–∏–ª—Å—è –∏–ª–∏ –µ–≥–æ –Ω–µ –±—ã–ª–æ ‚Äî –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º ===
        # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∫–∞–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏)
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        chunks = []
        current_chunk = []
        current_len = 0

        for para in paragraphs:
            para_len = token_len(para)
            if current_len + para_len > max_chunk_tokens and current_chunk:
                chunks.append('\n\n'.join(current_chunk))
                current_chunk = [para]
                current_len = para_len
            else:
                current_chunk.append(para)
                current_len += para_len
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))

        # === –ó–ê–©–ò–¢–ê –û–¢ –ü–£–°–¢–´–• –ß–ê–ù–ö–û–í ===
        # chunks = [c for c in chunks if c.strip()]
        if not chunks:
            logging.warning(f"{md_file.name}: –≤—Å–µ —á–∞–Ω–∫–∏ –ø—É—Å—Ç—ã–µ, —Ñ–∞–π–ª –ø—Ä–æ–ø—É—â–µ–Ω")
            continue

        total_tokens = sum(token_len(p) for p in paragraphs)
        logging.info(f"{md_file.name}: —á–∞–Ω–∫–æ–≤={len(chunks)}, —Ç–æ–∫–µ–Ω–æ–≤={total_tokens}, –º–æ–¥–µ–ª—å={model_name}")

        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–∞–Ω–∫–æ–≤
        chunk_records = []

        for i, chunk in enumerate(chunks):
            try:
                emb = ef([chunk])[0]
                emb = np.asarray(emb, dtype=np.float32)

                # L2-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ!)
                norm = np.linalg.norm(emb)
                if norm > 0:
                    emb /= norm

                chunk_records.append({
                    "chunk_id": i,
                    "tokens": token_len(chunk),
                    "text": chunk,
                    "embedding": emb,
                })

            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ —á–∞–Ω–∫–∞ {i} –≤ {md_file.name}: {e}")

        if not chunk_records:
            logging.warning(f"{md_file.name}: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–∞–Ω–∫–æ–≤")
            continue

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∏–ª–∏ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∑–∞–ø–∏—Å—å –ø–æ –¥–∞—Ç–µ
        result_dict[tradedate_str] = {
            "TRADEDATE": tradedate_str,
            "MD5_hash": md5_hash,
            "CHUNKS": chunk_records,
        }

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ markdown-—Ñ–∞–π–ª–∞
        file_end_time = time.perf_counter()
        file_total_time = file_end_time - file_start_time
        logging.info(f"{md_file.name}: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—Ä–µ–º—è: {file_total_time:.2f} —Å–µ–∫.")

    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –∏–∑ —Å–ª–æ–≤–∞—Ä—è ‚Äî –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ –¥–∞—Ç–µ
    df = pd.DataFrame(list(result_dict.values()), columns=["TRADEDATE", "MD5_hash", "CHUNKS"])
    df = df.sort_values("TRADEDATE").reset_index(drop=True)  # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
    logging.info(f"–°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, —Å—Ç—Ä–æ–∫: {len(df)}")
    return df

if __name__ == "__main__":
    start_time = time.perf_counter()  # ‚è±Ô∏è –°—Ç–∞—Ä—Ç —Ç–∞–π–º–µ—Ä–∞

    existing_df = load_existing_cache(cache_file)

    df_embeddings = build_embeddings_df(md_path, existing_df)

    end_time = time.perf_counter()  # ‚è±Ô∏è –ö–æ–Ω–µ—Ü —Ç–∞–π–º–µ—Ä–∞
    total_time = end_time - start_time

    print(len(df_embeddings))

    with pd.option_context(
        "display.width", 1000,
        "display.max_columns", 10,
        "display.max_colwidth", 100
    ):
        print("–î–∞—Ç–∞—Ñ—Ä–µ–π–º —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏:")
        print(df_embeddings)

    print("–ß–∞–Ω–∫–æ–≤ –≤ –ø–µ—Ä–≤–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ:", len(df_embeddings['CHUNKS'].iloc[0]))
    print("–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞:", len(df_embeddings['CHUNKS'].iloc[0][0]["embedding"]))

    try:
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        with open(cache_file, 'wb') as f:
            pickle.dump(df_embeddings, f)
        logging.info(f"–ö—ç—à –æ–±–Ω–æ–≤–ª—ë–Ω –≤ {cache_file}, –≤—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df_embeddings)}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫—ç—à–∞ –≤ {cache_file}: {str(e)}")

    # üìä –õ–æ–≥–∏—Ä—É–µ–º –æ–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    logging.info(f"‚úÖ –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à—ë–Ω. –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time:.2f} —Å–µ–∫.")
