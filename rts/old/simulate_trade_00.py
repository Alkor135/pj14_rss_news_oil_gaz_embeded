"""
–°–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–Ω–µ–≤–Ω—ã–µ –∫–æ—Ç–∏—Ä–æ–≤–∫–∏ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–æ–≤–æ—Å—Ç–µ–π, –æ–±—ä–µ–¥–∏–Ω—è—è –∏—Ö –ø–æ –¥–∞—Ç–∞–º.
–î–ª—è –∫–∞–∂–¥–æ–π –¥–∞—Ç—ã —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ç–µ–∫—É—â–∏–π –≤–µ–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–∏ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ k –¥–Ω—è–º–∏ (–æ—Ç 3 –¥–æ 30) —á–µ—Ä–µ–∑ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ.
–ù–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–π –¥–µ–Ω—å –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω–æ–≤–æ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è (NEXT_BODY).
–§–æ—Ä–º–∏—Ä—É–µ—Ç –ø—Ä–æ—Ñ–∏—Ç/–ª–æ—Å—Å: +|–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å| –ø—Ä–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π, –∏–Ω–∞—á–µ -|–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å|.
–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π P/L –∑–∞ test_days –¥–Ω–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–∫–Ω–∞ k –∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–µ–µ.
–°–∏–≥–Ω–∞–ª–æ–º –Ω–∞ –¥–µ–Ω—å t —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ MAX_k –∏–∑ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –æ–∫–Ω–∞, —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω–æ–µ –Ω–∞ –¥–µ–Ω—å t-1.
–°—Ç—Ä–æ–∏—Ç –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–π P/L –ø–æ —ç—Ç–∏–º —Å–∏–≥–Ω–∞–ª–∞–º –∏ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –ª—É—á—à–∏–µ –æ–∫–Ω–∞ (k) –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ.
–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–≤–æ–¥—è—Ç—Å—è –≤ –≤–∏–¥–µ —Ç–∞–±–ª–∏—Ü—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ —Å –¥–≤–æ–π–Ω–æ–π –æ—Å—å—é.
–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–∏–±–∫—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —á–µ—Ä–µ–∑ YAML-—Ñ–∞–π–ª.
"""

from pathlib import Path
from datetime import datetime
import pickle
import sqlite3
import logging
import yaml
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# –ü—É—Ç—å –∫ settings.yaml –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, —á—Ç–æ –∏ —Å–∫—Ä–∏–ø—Ç
SETTINGS_FILE = Path(__file__).parent / "settings.yaml"

_CHUNK_MATRIX_CACHE = {}  # –ö—ç—à –¥–ª—è –º–∞—Ç—Ä–∏—Ü —á–∞–Ω–∫–æ–≤
EXPLAIN_STORE = {}  # { k -> [ { trade_date, best_j_date, score, pairs, body_cur, body_prev } ] }

# –ß—Ç–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫
with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
    settings = yaml.safe_load(f)

# ==== –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ====
ticker = settings['ticker']
ticker_lc = ticker.lower()
cache_file = Path(settings['cache_file'].replace('{ticker_lc}', ticker_lc))  # –ü—É—Ç—å –∫ pkl-—Ñ–∞–π–ª—É —Å –∫—ç—à–µ–º
path_db_day = Path(settings['path_db_day'].replace('{ticker}', ticker))  # –ü—É—Ç—å –∫ –ë–î –¥–Ω–µ–≤–Ω—ã—Ö –∫–æ—Ç–∏—Ä–æ–≤–æ–∫
min_prev_files = settings.get('min_prev_files', 2)
test_days = settings.get('test_days', 23) + 1
START_DATE = settings.get('start_date_test', "2025-10-01")
model_name = settings.get('model_name', 'bge-m3')  # Ollama –º–æ–¥–µ–ª—å
provider = settings['provider']

# === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
log_dir = Path(__file__).parent / 'log'
log_dir.mkdir(parents=True, exist_ok=True)
# –ò–º—è —Ñ–∞–π–ª–∞ –ª–æ–≥–∞ —Å –¥–∞—Ç–æ–π –∏ –≤—Ä–µ–º–µ–Ω–µ–º –∑–∞–ø—É—Å–∫–∞ (–æ–¥–∏–Ω —Ñ–∞–π–ª –Ω–∞ –∑–∞–ø—É—Å–∫!)
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
log_file = log_dir / f'simulate_trade_{timestamp}.txt'

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: –¢–û–õ–¨–ö–û –æ–¥–∏–Ω —Ñ–∞–π–ª + –∫–æ–Ω—Å–æ–ª—å
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),  # –æ–¥–∏–Ω —Ñ–∞–π–ª
        logging.StreamHandler()                           # –∫–æ–Ω—Å–æ–ª—å
    ]
)

# –†—É—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ 3 —Å–∞–º—ã—Ö –Ω–æ–≤—ã—Ö)
def cleanup_old_logs(log_dir: Path, max_files: int = 3):
    """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –ª–æ–≥-—Ñ–∞–π–ª—ã, –æ—Å—Ç–∞–≤–ª—è—è max_files —Å–∞–º—ã—Ö –Ω–æ–≤—ã—Ö."""
    log_files = sorted(log_dir.glob("simulate_trade_*.txt"))
    if len(log_files) > max_files:
        for old_file in log_files[:-max_files]:
            try:
                old_file.unlink()
                print(f"–£–¥–∞–ª—ë–Ω —Å—Ç–∞—Ä—ã–π –ª–æ–≥: {old_file.name}")
            except Exception as e:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {old_file}: {e}")

# –í—ã–∑—ã–≤–∞–µ–º –æ—á–∏—Å—Ç–∫—É –ü–ï–†–ï–î –Ω–∞—á–∞–ª–æ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
cleanup_old_logs(log_dir, max_files=3)
logging.info(f"üöÄ –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞. –õ–æ–≥-—Ñ–∞–π–ª: {log_file}")

def load_quotes(path_db_quote):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ç–∏—Ä–æ–≤–æ–∫ –∏ —Ä–∞—Å—á–µ—Ç NEXT_BODY."""
    with sqlite3.connect(path_db_quote) as conn:
        df = pd.read_sql_query(
            "SELECT TRADEDATE, OPEN, CLOSE FROM Futures",
            conn,
            parse_dates=['TRADEDATE']  # <-- –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º TRADEDATE –≤ datetime
        )
    df = df.set_index('TRADEDATE').sort_index()
    df['NEXT_BODY'] = (df['CLOSE'] - df['OPEN']).shift(-1)
    df = df.dropna(subset=['NEXT_BODY'])
    return df[['NEXT_BODY']]

def load_cache(cache_file_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."""
    with open(cache_file_path, 'rb') as f:
        df = pickle.load(f)
    df['TRADEDATE'] = pd.to_datetime(df['TRADEDATE'])
    return df.set_index('TRADEDATE').sort_index()

def chunks_to_matrix(chunks):
    key = id(chunks)
    if key not in _CHUNK_MATRIX_CACHE:
        _CHUNK_MATRIX_CACHE[key] = np.vstack(
            [c["embedding"] for c in chunks]
        ).astype(np.float32)
    return _CHUNK_MATRIX_CACHE[key]

def cosine(a: np.ndarray, b: np.ndarray) -> float:
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É"""
    # —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —É–∂–µ L2-–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã
    return float(np.dot(a, b))

def chunks_similarity_fast(
    chunks_a: list,
    chunks_b: list,
    top_k: int = 5
) -> float:
    """    –ë—ã—Å—Ç—Ä–æ–µ retriever-grade similarity —á–µ—Ä–µ–∑ –º–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ    """

    if not chunks_a or not chunks_b:
        return 0.0

    A = chunks_to_matrix(chunks_a)  # (Na, D)
    B = chunks_to_matrix(chunks_b)  # (Nb, D)

    # –í—Å–µ cosine similarity —Å—Ä–∞–∑—É
    S = A @ B.T  # (Na, Nb)

    # top-k –ø–æ –≤—Å–µ–º –∑–Ω–∞—á–µ–Ω–∏—è–º
    flat = S.ravel()

    if flat.size <= top_k:
        return float(flat.mean())

    # –±—ã—Å—Ç—Ä–µ–µ —á–µ–º sort
    top = np.partition(flat, -top_k)[-top_k:]
    return float(top.mean())

def chunks_similarity_with_explain(
    chunks_a: list,
    chunks_b: list,
    top_k: int = 5
):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (score, pairs) –≥–¥–µ pairs ‚Äî —Å–ø–∏—Å–æ–∫ —Ç–æ–ø-k —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π:
    [{"chunk_a": idx_a, "chunk_b": idx_b, "similarity": float,
      "text_a": "...", "text_b": "..."}...]
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–∞—Ç—Ä–∏—á–Ω—É—é –æ–ø–µ—Ä–∞—Ü–∏—é, –±—ã—Å—Ç—Ä–æ.
    """
    if not chunks_a or not chunks_b:
        return 0.0, []

    A = chunks_to_matrix(chunks_a)  # (Na, D)
    B = chunks_to_matrix(chunks_b)  # (Nb, D)

    S = A @ B.T  # (Na, Nb)

    flat = S.ravel()
    total = flat.size
    k = min(top_k, total)

    if total == 0:
        return 0.0, []

    # get indices of top-k in flattened array
    if total <= k:
        top_idx = np.arange(total)
    else:
        top_idx = np.argpartition(flat, -k)[-k:]

    # build readable pairs
    Nb = B.shape[0]
    pairs = []
    # sort top_idx by actual similarity descending for readability
    top_idx_sorted = top_idx[np.argsort(flat[top_idx])[::-1]]

    for idx in top_idx_sorted:
        ia = int(idx // Nb)
        ib = int(idx % Nb)
        sim = float(S[ia, ib])
        text_a = chunks_a[ia].get("text", "") if isinstance(chunks_a[ia], dict) else ""
        text_b = chunks_b[ib].get("text", "") if isinstance(chunks_b[ib], dict) else ""
        pairs.append({
            "chunk_a": ia,
            "chunk_b": ib,
            "similarity": sim,
            "text_a": text_a,
            "text_b": text_b
        })

    score = float(np.mean([p["similarity"] for p in pairs])) if pairs else 0.0
    return score, pairs

def compute_max_k(
    df: pd.DataFrame,
    start_date: pd.Timestamp,
    k: int,
    col_chunks: str = "CHUNKS",
    col_body: str = "NEXT_BODY",
    top_k_chunks: int = 5
) -> pd.Series:

    result = pd.Series(index=df.index, dtype=float)

    dates = df.index
    start_pos = dates.get_loc(start_date)

    # ensure explain list exists for this k
    EXPLAIN_STORE.setdefault(k, [])

    for i in range(start_pos, len(df)):
        if i < k:
            continue

        chunks_cur = df.iloc[i][col_chunks]
        body_cur = df.iloc[i][col_body]

        similarities = []
        indices = []

        # –±—ã—Å—Ç—Ä—ã–µ —Å–∏–º—ã –¥–ª—è –≤—ã–±–æ—Ä–∞ best_j
        for j in range(i - k, i):
            chunks_prev = df.iloc[j][col_chunks]

            sim = chunks_similarity_fast(
                chunks_cur,
                chunks_prev,
                top_k=top_k_chunks
            )

            similarities.append(sim)
            indices.append(j)

        # –∏–Ω–¥–µ–∫—Å —Å–∞–º–æ–π –ø–æ—Ö–æ–∂–µ–π —Å—Ç—Ä–æ–∫–∏
        best_idx = int(np.argmax(similarities))
        best_j = indices[best_idx]
        body_prev = df.iloc[best_j][col_body]

        # --- –∑–∞–ø–∏—Å—ã–≤–∞–µ–º explain —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ best_j ---
        try:
            score, pairs = chunks_similarity_with_explain(
                chunks_cur,
                df.iloc[best_j][col_chunks],
                top_k=top_k_chunks
            )
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ explain –¥–ª—è {dates[i]} vs {dates[best_j]}: {e}")
            score, pairs = similarities[best_idx], []

        EXPLAIN_STORE[k].append({
            "trade_date": dates[i],
            "best_j_date": dates[best_j],
            "score": float(similarities[best_idx]),
            "explained_score": float(score),
            "pairs": pairs,
            "body_cur": float(body_cur),
            "body_prev": float(body_prev)
        })
        # --------------------------------------------------------

        if np.sign(body_cur) == np.sign(body_prev):
            result.iloc[i] = abs(body_cur)
        else:
            result.iloc[i] = -abs(body_cur)

    return result
    # return result *= -1


def main(path_db_day, cache_file):
    df_bar = load_quotes(path_db_day)  # –ó–∞–≥—Ä—É–∑–∫–∞ DF —Å –¥–Ω–µ–≤–Ω—ã–º–∏ –∫–æ—Ç–∏—Ä–æ–≤–∫–∞–º–∏ (—Å 21:00 –ø—Ä–µ–¥. —Å–µ—Å—Å–∏–∏)
    df_emb = load_cache(cache_file)  # –ó–∞–≥—Ä—É–∑–∫–∞ DF —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –Ω–æ–≤–æ—Å—Ç–µ–π

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤ –ø–æ –∏–Ω–¥–µ–∫—Å—É TRADEDATE
    df_combined = df_bar.join(df_emb[['CHUNKS']], how='inner')  # 'inner' ‚Äî —Ç–æ–ª—å–∫–æ –æ–±—â–∏–µ –¥–∞—Ç—ã

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–ª–æ–Ω–æ–∫ MAX_3 ‚Ä¶ MAX_30
    start_date = pd.to_datetime(START_DATE)
    for k in range(3, 31):
        col_name = f"MAX_{k}"
        logging.info(f"üìä –†–∞—Å—á—ë—Ç {col_name}")
        df_combined[col_name] = compute_max_k(
            df=df_combined,
            start_date=start_date,
            k=k
        )

    # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º explain-—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ pickle –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ ---
    explain_dir = cache_file.parent
    explain_path = explain_dir / f"explain_topk_all.pkl"  # _{timestamp}
    try:
        with open(explain_path, "wb") as ef:
            pickle.dump(EXPLAIN_STORE, ef)
        logging.info(f"üîç Explain saved: {explain_path}")
    except Exception as e:
        logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å explain: {e}")

    # debug: –ø–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã explain –¥–ª—è k=5, –µ—Å–ª–∏ –µ—Å—Ç—å
    example_k = 5
    if example_k in EXPLAIN_STORE and EXPLAIN_STORE[example_k]:
        sample = EXPLAIN_STORE[example_k][-1]  # –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–π –¥–µ–Ω—å
        logging.info(
            f"–ü—Ä–∏–º–µ—Ä explain –¥–ª—è k={example_k}: trade_date={sample['trade_date']}, "
            f"best_j={sample['best_j_date']}, score={sample['score']}")
        for p in sample["pairs"][:5]:
            logging.info(
                f"  sim={p['similarity']:.4f} | A[{p['chunk_a']}]='{p['text_a'][:120]}' -> "
                f"B[{p['chunk_b']}]='{p['text_b'][:120]}'")

    # === –ó–∞–º–µ–Ω–∞ NaN –Ω–∞ 0.0 –≤–æ –≤—Å–µ—Ö MAX_ –∫–æ–ª–æ–Ω–∫–∞—Ö ===
    max_cols = [f"MAX_{k}" for k in range(3, 31)]
    df_combined[max_cols] = df_combined[max_cols].fillna(0.0)

    # === –†–∞—Å—á—ë—Ç PL_ –∫–æ–ª–æ–Ω–æ–∫ ===
    for k in range(3, 31):
        max_col = f"MAX_{k}"
        pl_col = f"PL_{k}"

        df_combined[pl_col] = (
            df_combined[max_col]
            .shift(1)  # –∏—Å–∫–ª—é—á–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–æ–∫—É
            .rolling(window=test_days, min_periods=1)
            .sum()
        )

    # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥
    with pd.option_context(
        "display.width", 1000,
        "display.max_columns", 10,
        "display.max_colwidth", 120
    ):
        print(df_bar)
        print(df_emb)
        print(df_combined[["NEXT_BODY", "CHUNKS"]])
        print(df_combined)

    # === –ó–∞–º–µ–Ω–∞ NaN –Ω–∞ 0.0 –≤–æ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö ===
    df_combined = df_combined.fillna(0.0)

    # === –û–°–¢–ê–í–ò–¢–¨ –¢–û–õ–¨–ö–û –ù–£–ñ–ù–´–ï –ö–û–õ–û–ù–ö–ò ===
    final_cols = [f"MAX_{k}" for k in range(3, 31)] + [f"PL_{k}" for k in range(3, 31)]
    df_combined = df_combined[final_cols].copy()

    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∏–Ω–¥–µ–∫—Å—É (–ø–æ –¥–∞—Ç–µ)
    df_combined.sort_index(inplace=True)

    # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥
    with pd.option_context(
        "display.width", 1000,
        "display.max_columns", 24,
        "display.max_colwidth", 120,
        "display.min_rows", 30
    ):
        print("\n–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DataFrame (df_combined) —Å MAX_ –∏ PL_ –∫–æ–ª–æ–Ω–∫–∞–º–∏:")
        print(df_combined[[f"PL_{k}" for k in range(3, 31)]])

    # ===============================
    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ df_rez
    # ===============================

    pl_cols = [f"PL_{k}" for k in range(3, 31)]
    max_cols = [f"MAX_{k}" for k in range(3, 31)]

    rows = []

    for idx, row in df_combined.iterrows():
        trade_date = idx

        # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å—Ä–µ–¥–∏ PL_3 ... PL_30
        pl_values = row[pl_cols]
        pl_max = pl_values.max()

        pl_result = 0.0

        # ---
        # if pl_max > 0.0:
        # –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º PL
        best_pl_col = pl_values.idxmax()  # –Ω–∞–ø—Ä–∏–º–µ—Ä "PL_7"
        n = int(best_pl_col.split("_")[1])  # -> 7

        # —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –∫–æ–ª–æ–Ω–∫–∞ MAX_n
        max_col = f"MAX_{n}"
        pl_result = row[max_col]
        # ---

        rows.append({
            "TRADEDATE": trade_date,
            "P/L": pl_result,
            "max": n
        })

    df_rez = pd.DataFrame(rows).set_index("TRADEDATE")

    # ===============================
    # –í—ã–≤–æ–¥ df_rez –≤ –∫–æ–Ω—Å–æ–ª—å
    # ===============================
    with pd.option_context(
            "display.width", 1000,
            "display.max_columns", 10,
            "display.max_colwidth", 120
    ):
        print("\n–†–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–π DataFrame (df_rez):")
        print(df_rez)

    # # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ DataFrame –≤ Excel —Ñ–∞–π–ª
    # df_rez.to_excel('df_rez_output.xlsx', index=False)

    # ===============================
    # –ì—Ä–∞—Ñ–∏–∫ cumulative P/L + –Ω–∞–ª–æ–∂–µ–Ω–Ω–∞—è —Å—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ max
    # ===============================

    # # --- –ó–ï–†–ö–ê–õ–¨–ù–û–ï –û–¢–û–ë–†–ê–ñ–ï–ù–ò–ï ---
    # df_rez["P/L"] *= -1
    # # -------------------------------

    df_rez["CUM_P/L"] = df_rez["P/L"].cumsum()

    fig, ax1 = plt.subplots(figsize=(12, 7))

    # –û—Å–Ω–æ–≤–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫: Cumulative P/L (—Å–ø—Ä–∞–≤–∞)
    ax1.plot(
        df_rez.index, df_rez["CUM_P/L"],
        marker='o',
        markersize=4,
        color='tab:blue',
        label='Cumulative P/L'
    )
    ax1.set_ylabel("Cumulative P/L", color='tab:blue')
    ax1.tick_params(axis='y', labelcolor='tab:blue')
    ax1.set_xlabel("Date")
    ax1.grid(True, axis='y', alpha=0.3)
    ax1.set_title(f"Cumulative P/L & Best Window (k) {model_name.split(':')[0]} {timestamp}")

    # –í—Ç–æ—Ä–∞—è –æ—Å—å Y –¥–ª—è —Å—Ç–æ–ª–±—á–∞—Ç–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã (—Å–ª–µ–≤–∞)
    ax2 = ax1.twinx()
    ax2.bar(
        df_rez.index, df_rez["max"],
        alpha=0.5,
        color='tab:green',
        width=0.5,
        label="Best Window (k)"
    )
    ax2.set_ylabel("Best Window (k)", color='tab:green')
    ax2.tick_params(axis='y', labelcolor='tab:green')
    ax2.set_ylim(df_rez["max"].min() - 1, df_rez["max"].max() + 1)

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ª–µ–≥–µ–Ω–¥—ã
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')

    # –û—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –æ—Å–∏ X
    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
    fig.tight_layout()

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
    plot_dir = Path(__file__).parent / 'plots'
    plot_dir.mkdir(exist_ok=True)
    plot_path = plot_dir / f'{model_name.split(":")[0]}_{provider}_{timestamp}.png'
    plt.savefig(plot_path)
    logging.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {plot_path}")
    plt.close()  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å

if __name__ == "__main__":
    main(path_db_day, cache_file)